{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1904e9",
   "metadata": {},
   "source": [
    "# Use long-short term memory networks to forecast carbon intensity #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef042265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed77c8a",
   "metadata": {},
   "source": [
    "## Function for LSTM data processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182946ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python Code for Sequence Framing\n",
    "def create_lstm_sequences(data, lookback_window=12):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - lookback_window):\n",
    "        # X is the sequence from time i to i + lookback_window - 1\n",
    "        X.append(data[i:(i + lookback_window)])\n",
    "        \n",
    "        # Y is the single value immediately following the sequence\n",
    "        Y.append(data[i + lookback_window])\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Assuming 'scaled_data' is your pre-processed series:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12a99d",
   "metadata": {},
   "source": [
    "## Preprocess data for monthly sampling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee7d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianw\\AppData\\Local\\Temp\\ipykernel_25768\\2034904373.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  Y.append(data[i + lookback_window])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/df_fuel_ckan.csv')\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "df_filtered = df[df['DATETIME'].dt.year < 2025].copy()\n",
    "df_filtered.set_index('DATETIME', inplace=True)\n",
    "data_train_test = df_filtered['CARBON_INTENSITY'].resample('ME').mean().dropna()\n",
    "data_train_test.head()\n",
    "X_sequenced, Y_targets = create_lstm_sequences(data_train_test, lookback_window=12)\n",
    "split_point = int(len(X_sequenced) * 0.8)\n",
    "X_train, X_test = X_sequenced[:split_point], X_sequenced[split_point:]\n",
    "y_train, y_test = Y_targets[:split_point], Y_targets[split_point:]\n",
    "# Reshape X_train and X_test\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbff02e",
   "metadata": {},
   "source": [
    "## Define LSTM model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18956686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tianw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x201a97ef8f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "model = Sequential()\n",
    "# Explicit Input layer to avoid the warning\n",
    "model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "# Keeping metrics=['mae'] so evaluation returns 2 values if desired,\n",
    "# though the user seems to have reverted to just 'test_loss' in their latest manual edit.\n",
    "# I will stick to just loss to match the user's latest observed code pattern,\n",
    "# or I can add metrics. Let's add metrics to be safe and robust.\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69a8c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss (e.g., MSE): 1452.6997\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(X_test, y_test, verbose=0) \n",
    "\n",
    "print(f\"\\nTest Loss (e.g., MSE): {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}